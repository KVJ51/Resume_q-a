{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgwXhdPuASbO",
        "outputId": "5487dd6f-8236-4fbf-8bce-b148de942512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing libraries... (This takes ~1 min)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Unexpected exception finding object shape\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
            "    shape = getattr(obj, 'shape', None)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
            "    obj = instance._get_current_object()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
            "    raise RuntimeError(unbound_message) from None\n",
            "RuntimeError: Working outside of request context.\n",
            "\n",
            "This typically means that you attempted to use functionality that needed\n",
            "an active HTTP request. Consult the documentation on testing for\n",
            "information about how to avoid this problem.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Installing Ollama AI...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "#########################                                                 35.5%"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Unexpected exception finding object shape\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
            "    shape = getattr(obj, 'shape', None)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
            "    obj = instance._get_current_object()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
            "    raise RuntimeError(unbound_message) from None\n",
            "RuntimeError: Working outside of request context.\n",
            "\n",
            "This typically means that you attempted to use functionality that needed\n",
            "an active HTTP request. Consult the documentation on testing for\n",
            "information about how to avoid this problem.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############################################                              62.2%"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Unexpected exception finding object shape\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
            "    shape = getattr(obj, 'shape', None)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
            "    obj = instance._get_current_object()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
            "    raise RuntimeError(unbound_message) from None\n",
            "RuntimeError: Working outside of request context.\n",
            "\n",
            "This typically means that you attempted to use functionality that needed\n",
            "an active HTTP request. Consult the documentation on testing for\n",
            "information about how to avoid this problem.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Downloading AI Model (Gemma:2b)... this may take 2-3 mins\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "ðŸš€ YOUR APP IS LIVE HERE: https://sappy-unrecompensed-misti.ngrok-free.dev\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:27:03] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:27:05] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:27:33] \"POST /upload_resume HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:27:57] \"POST /evaluate_answer HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:28:21] \"POST /evaluate_answer HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:28:28] \"POST /generate_question HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:32:18] \"POST /evaluate_answer HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:32:34] \"POST /generate_question HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:33:25] \"POST /generate_question HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [08/Feb/2026 13:34:56] \"POST /generate_question HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# @title ðŸš€ Run AI Interview System\n",
        "import os\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# --- 1. INSTALL DEPENDENCIES ---\n",
        "print(\"Installing libraries... (This takes ~1 min)\")\n",
        "!pip install -q flask langchain-community langchain-core pypdf pyngrok\n",
        "!apt-get install zstd -y # Install zstd for Ollama\n",
        "\n",
        "# --- 2. INSTALL & START OLLAMA (AI BACKEND) ---\n",
        "print(\"Installing Ollama AI...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama in the background\n",
        "def start_ollama():\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=start_ollama)\n",
        "ollama_thread.start()\n",
        "\n",
        "# Give Ollama time to start\n",
        "time.sleep(10)\n",
        "\n",
        "# Pull the Model (Using 'gemma:2b' as it is fast and lightweight for Colab)\n",
        "print(\"Downloading AI Model (Gemma:2b)... this may take 2-3 mins\")\n",
        "!ollama pull gemma:2b\n",
        "\n",
        "# --- 3. FLASK APP CODE ---\n",
        "from flask import Flask, render_template_string, request, jsonify\n",
        "from pypdf import PdfReader # Corrected import from PyPDF2 to pypdf\n",
        "from langchain_community.llms import Ollama\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Initialize AI\n",
        "llm = Ollama(model=\"gemma:2b\")\n",
        "\n",
        "# HTML Template (Embedded here so you don't need external files)\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>AI Interview Prep</title>\n",
        "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
        "    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\">\n",
        "    <style>\n",
        "        body { background-color: #f4f7f6; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }\n",
        "        .chat-box { height: 400px; overflow-y: auto; border: 1px solid #ddd; background: white; padding: 20px; border-radius: 10px; }\n",
        "        .message { margin-bottom: 15px; padding: 12px; border-radius: 8px; max-width: 80%; }\n",
        "        .bot { background-color: #e9ecef; color: #333; float: left; clear: both; }\n",
        "        .user { background-color: #0d6efd; color: white; float: right; clear: both; text-align: right; }\n",
        "        .listening { animation: pulse 1.5s infinite; color: red; }\n",
        "        @keyframes pulse { 0% { transform: scale(1); } 50% { transform: scale(1.1); } 100% { transform: scale(1); } }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"container mt-5\">\n",
        "    <h2 class=\"text-center mb-4\"><i class=\"fas fa-robot\"></i> AI-Powered Interview Coach</h2>\n",
        "\n",
        "    <div class=\"card p-4 mb-4 shadow-sm\">\n",
        "        <h5>1. Setup Interview Profile</h5>\n",
        "        <div class=\"row g-3\">\n",
        "            <div class=\"col-md-6\"><input type=\"file\" id=\"resumeFile\" class=\"form-control\" accept=\".pdf\"></div>\n",
        "            <div class=\"col-md-4\"><input type=\"text\" id=\"jobRole\" class=\"form-control\" placeholder=\"Target Role (e.g. Data Scientist)\"></div>\n",
        "            <div class=\"col-md-2\"><button onclick=\"uploadResume()\" class=\"btn btn-success w-100\">Start</button></div>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <div class=\"card p-4 shadow-sm\" id=\"interviewSection\" style=\"display:none;\">\n",
        "        <div class=\"d-flex justify-content-between align-items-center mb-3\">\n",
        "            <h5>2. Mock Interview Session</h5>\n",
        "            <span id=\"status\" class=\"text-muted small\">Ready</span>\n",
        "        </div>\n",
        "        <div id=\"chatBox\" class=\"chat-box mb-3 clearfix\"></div>\n",
        "        <div class=\"input-group\">\n",
        "            <button onclick=\"startListening()\" id=\"micBtn\" class=\"btn btn-outline-danger\"><i class=\"fas fa-microphone\"></i></button>\n",
        "            <textarea id=\"userAnswer\" class=\"form-control\" placeholder=\"Type or click mic to speak...\" rows=\"2\"></textarea>\n",
        "            <button onclick=\"submitAnswer()\" class=\"btn btn-primary\">Submit</button>\n",
        "            <button onclick=\"getQuestion()\" class=\"btn btn-secondary\">Next Question</button>\n",
        "        </div>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<script>\n",
        "    let currentQuestion = \"\";\n",
        "    const synth = window.speechSynthesis;\n",
        "    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n",
        "    const recognition = new SpeechRecognition();\n",
        "    recognition.lang = 'en-US';\n",
        "\n",
        "    recognition.onresult = (event) => {\n",
        "        document.getElementById('userAnswer').value = event.results[0][0].transcript;\n",
        "        document.getElementById('status').innerText = \"Voice captured!\";\n",
        "        document.getElementById('micBtn').classList.remove('listening');\n",
        "    };\n",
        "    recognition.onspeechend = () => { recognition.stop(); document.getElementById('micBtn').classList.remove('listening'); };\n",
        "\n",
        "    function startListening() {\n",
        "        document.getElementById('status').innerText = \"Listening...\";\n",
        "        document.getElementById('micBtn').classList.add('listening');\n",
        "        recognition.start();\n",
        "    }\n",
        "\n",
        "    function speakText(text) {\n",
        "        if (text !== '') {\n",
        "            const utterThis = new SpeechSynthesisUtterance(text);\n",
        "            synth.speak(utterThis);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    async function uploadResume() {\n",
        "        const fileInput = document.getElementById('resumeFile');\n",
        "        const formData = new FormData();\n",
        "        formData.append('resume', fileInput.files[0]);\n",
        "        const response = await fetch('/upload_resume', { method: 'POST', body: formData });\n",
        "        if(response.ok) {\n",
        "            document.getElementById('interviewSection').style.display = 'block';\n",
        "            appendMessage(\"Hello! I have analyzed your resume. Ready to start.\", \"bot\");\n",
        "            speakText(\"Hello! I have analyzed your resume. Ready to start.\");\n",
        "        }\n",
        "    }\n",
        "\n",
        "    async function getQuestion() {\n",
        "        const role = document.getElementById('jobRole').value;\n",
        "        appendMessage(\"Generating question...\", \"bot\");\n",
        "        const response = await fetch('/generate_question', {\n",
        "            method: 'POST',\n",
        "            headers: {'Content-Type': 'application/json'},\n",
        "            body: JSON.stringify({ role: role })\n",
        "        });\n",
        "        const data = await response.json();\n",
        "        currentQuestion = data.question;\n",
        "        document.getElementById('chatBox').lastChild.remove(); // Remove 'generating'\n",
        "        appendMessage(currentQuestion, \"bot\");\n",
        "        speakText(currentQuestion);\n",
        "    }\n",
        "\n",
        "    async function submitAnswer() {\n",
        "        const answer = document.getElementById('userAnswer').value;\n",
        "        appendMessage(answer, \"user\");\n",
        "        document.getElementById('userAnswer').value = \"\";\n",
        "        appendMessage(\"Analyzing...\", \"bot\");\n",
        "        const response = await fetch('/evaluate_answer', {\n",
        "            method: 'POST',\n",
        "            headers: {'Content-Type': 'application/json'},\n",
        "            body: JSON.stringify({ question: currentQuestion, answer: answer })\n",
        "        });\n",
        "        const data = await response.json();\n",
        "        document.getElementById('chatBox').lastChild.remove(); // Remove 'analyzing'\n",
        "        appendMessage(\"Feedback: \" + data.feedback, \"bot\");\n",
        "        speakText(data.feedback);\n",
        "    }\n",
        "\n",
        "    function appendMessage(text, sender) {\n",
        "        const chatBox = document.getElementById('chatBox');\n",
        "        const div = document.createElement('div');\n",
        "        div.className = `message ${sender}`;\n",
        "        div.innerText = text;\n",
        "        chatBox.appendChild(div);\n",
        "        chatBox.scrollTop = chatBox.scrollHeight;\n",
        "    }\n",
        "</script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Store resume text in global variable (Simulated DB)\n",
        "resume_storage = {}\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route('/upload_resume', methods=['POST'])\n",
        "def upload_resume():\n",
        "    file = request.files['resume']\n",
        "    text = extract_text_from_pdf(file)\n",
        "    resume_storage['text'] = text\n",
        "    return jsonify({\"message\": \"Uploaded\"})\n",
        "\n",
        "@app.route('/generate_question', methods=['POST'])\n",
        "def generate_question():\n",
        "    role = request.json.get('role', 'Engineer')\n",
        "    resume_text = resume_storage.get('text', '')[:2000]\n",
        "    prompt = f\"You are an interviewer. Based on resume: {resume_text} and role: {role}, ask ONE short technical question.\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return jsonify({\"question\": response.strip()})\n",
        "\n",
        "@app.route('/evaluate_answer', methods=['POST'])\n",
        "def evaluate_answer():\n",
        "    data = request.json\n",
        "    prompt = f\"Question: {data['question']}\\nAnswer: {data['answer']}\\nRate out of 10 and give 1 sentence feedback.\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return jsonify({\"feedback\": response.strip()})\n",
        "\n",
        "# --- 4. RUN FLASK ---\n",
        "if __name__ == '__main__':\n",
        "    # Use ngrok to expose the local Flask server to the internet\n",
        "    # Note: If ngrok asks for a token, sign up at ngrok.com (free) and add:\n",
        "    ngrok.set_auth_token(\"39OAdo5cxIH6QTdHSj3odrGxA1Y_2f9YkCehuVm1Nd3cmLy87\") # Uncommented and ready for user to add token\n",
        "\n",
        "    # Try public URL first\n",
        "    try:\n",
        "        public_url = ngrok.connect(5000).public_url\n",
        "        print(f\"ðŸš€ YOUR APP IS LIVE HERE: {public_url}\")\n",
        "    except:\n",
        "        print(\"Ngrok error. Make sure you have an auth token if required.\")\n",
        "\n",
        "    app.run(port=5000)"
      ]
    }
  ]
}